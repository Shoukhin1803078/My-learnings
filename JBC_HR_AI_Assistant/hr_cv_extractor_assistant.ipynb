{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-generativeai\n",
    "!pip install pandas\n",
    "!pip install PyPDF2\n",
    "!pip install openai\n",
    "!pip install python-dateutil\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV Analyzer for Google Drive - JBC HR AI Assistant (Modified for Gemini API)\n",
    "# Mount Google Drive, extract CV information from PDFs, and create an Excel output file\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import io\n",
    "import PyPDF2\n",
    "import google.generativeai as genai\n",
    "import re\n",
    "import tempfile\n",
    "import json\n",
    "from datetime import datetime\n",
    "import dateutil.parser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from google.colab import drive\n",
    "from tqdm.notebook import tqdm\n",
    "from googleapiclient.discovery import build\n",
    "from google.colab import auth\n",
    "from google.auth import default\n",
    "import openpyxl\n",
    "from openpyxl.styles import Font\n",
    "from openpyxl.styles.colors import Color\n",
    "from google.colab import files\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set your Gemini API key\n",
    "gemini_api_key = \"\"  # Replace with your actual Gemini API key\n",
    "genai.configure(api_key=gemini_api_key)\n",
    "\n",
    "# Initialize Gemini model\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "\n",
    "# Define the root path\n",
    "root_path = \"/content/drive/MyDrive/JBC HR AI Assistant /23.04\"\n",
    "\n",
    "# Google Drive file link base URL\n",
    "DRIVE_LINK_BASE = \"https://drive.google.com/file/d/\"\n",
    "\n",
    "# Function to get Google Drive file ID\n",
    "def get_file_id(file_path):\n",
    "    \"\"\"Get Google Drive file ID from local path\"\"\"\n",
    "    try:\n",
    "        # Authenticate and create the Drive API client\n",
    "        auth.authenticate_user()\n",
    "        creds, _ = default()\n",
    "        drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "        # Extract the relative path from the full path\n",
    "        relative_path = os.path.relpath(file_path, \"/content/drive/MyDrive\")\n",
    "\n",
    "        # Search for the file by name\n",
    "        filename = os.path.basename(file_path)\n",
    "        query = f\"name = '{filename}' and trashed = false\"\n",
    "\n",
    "        # Execute the query\n",
    "        results = drive_service.files().list(\n",
    "            q=query,\n",
    "            spaces='drive',\n",
    "            fields='files(id, name, parents)'\n",
    "        ).execute()\n",
    "\n",
    "        items = results.get('files', [])\n",
    "\n",
    "        if not items:\n",
    "            return None\n",
    "\n",
    "        # If multiple files have the same name, try to match the path\n",
    "        if len(items) > 1:\n",
    "            for item in items:\n",
    "                # Get the file's complete path\n",
    "                file_path_in_drive = get_file_path_in_drive(drive_service, item['id'])\n",
    "                if relative_path in file_path_in_drive:\n",
    "                    return item['id']\n",
    "\n",
    "            # If no path match, return the first one\n",
    "            return items[0]['id']\n",
    "        else:\n",
    "            return items[0]['id']\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting file ID: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Function to get file path in Drive\n",
    "def get_file_path_in_drive(service, file_id):\n",
    "    \"\"\"Get the file path in Google Drive\"\"\"\n",
    "    try:\n",
    "        # Get the file metadata\n",
    "        file = service.files().get(fileId=file_id, fields='name, parents').execute()\n",
    "\n",
    "        path = [file['name']]\n",
    "\n",
    "        # Get all parent folders\n",
    "        if 'parents' in file:\n",
    "            parent_id = file['parents'][0]\n",
    "            while parent_id:\n",
    "                parent = service.files().get(fileId=parent_id, fields='name, parents').execute()\n",
    "                path.insert(0, parent['name'])\n",
    "\n",
    "                if 'parents' in parent:\n",
    "                    parent_id = parent['parents'][0]\n",
    "                else:\n",
    "                    parent_id = None\n",
    "\n",
    "        return '/'.join(path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting file path: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to create shareable link\n",
    "def create_shareable_link(file_id):\n",
    "    \"\"\"Create a shareable link for the Google Drive file\"\"\"\n",
    "    if file_id:\n",
    "        return f\"{DRIVE_LINK_BASE}{file_id}/view?usp=sharing\"\n",
    "    return \"Link not available\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    \"\"\"Extract text content from a PDF file.\"\"\"\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "        text += pdf_reader.pages[page_num].extract_text()\n",
    "    return text\n",
    "\n",
    "def calculate_experience_duration(start_date_str):\n",
    "    \"\"\"Calculate duration between start date and current date in 'X year Y month' format.\"\"\"\n",
    "    try:\n",
    "        # Parse the start date string\n",
    "        if start_date_str == \"Not found\" or not start_date_str:\n",
    "            return \"Not found\"\n",
    "\n",
    "        # Try to parse the date with dateutil parser\n",
    "        try:\n",
    "            start_date = dateutil.parser.parse(start_date_str, fuzzy=True)\n",
    "        except:\n",
    "            # If parsing fails, try to extract month and year manually\n",
    "            match = re.search(r'(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* (\\d{4})',\n",
    "                             start_date_str, re.IGNORECASE)\n",
    "            if match:\n",
    "                month_str = match.group(1)\n",
    "                year_str = match.group(2)\n",
    "                # Map abbreviated month to number\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4,\n",
    "                    'may': 5, 'jun': 6, 'jul': 7, 'aug': 8,\n",
    "                    'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                month = month_map.get(month_str.lower()[:3], 1)\n",
    "                year = int(year_str)\n",
    "                start_date = datetime(year, month, 1)\n",
    "            else:\n",
    "                return \"Date format not recognized\"\n",
    "\n",
    "        # Calculate the difference between the start date and current date\n",
    "        current_date = datetime.now()\n",
    "        delta = relativedelta(current_date, start_date)\n",
    "\n",
    "        # Format the result as \"X year Y month\"\n",
    "        years = delta.years\n",
    "        months = delta.months\n",
    "\n",
    "        if years == 0:\n",
    "            if months == 1:\n",
    "                return f\"{months} month\"\n",
    "            else:\n",
    "                return f\"{months} months\"\n",
    "        elif years == 1:\n",
    "            if months == 0:\n",
    "                return \"1 year\"\n",
    "            elif months == 1:\n",
    "                return \"1 year 1 month\"\n",
    "            else:\n",
    "                return f\"1 year {months} months\"\n",
    "        else:\n",
    "            if months == 0:\n",
    "                return f\"{years} years\"\n",
    "            elif months == 1:\n",
    "                return f\"{years} years 1 month\"\n",
    "            else:\n",
    "                return f\"{years} years {months} months\"\n",
    "    except Exception as e:\n",
    "        return f\"Error calculating duration: {str(e)}\"\n",
    "\n",
    "def extract_field(text, field_name):\n",
    "    \"\"\"Extract a specific field from text response when JSON parsing fails\"\"\"\n",
    "    pattern = rf\"{field_name}[:\\s]+(.*?)(?:\\n|$|,)\"\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return \"Not found\"\n",
    "\n",
    "def extract_cv_info(cv_text):\n",
    "    \"\"\"Use Gemini API to extract structured information from CV text.\"\"\"\n",
    "\n",
    "    # Get current date for calculating work experience\n",
    "    current_date = datetime.now()\n",
    "    current_date_str = current_date.strftime(\"%Y %B\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Extract the following information from the CV text below.\n",
    "    If you cannot find a particular piece of information, respond with \"Not found\" for that field.\n",
    "\n",
    "    Information to extract:\n",
    "    1. Name\n",
    "    2. Last Education and university\n",
    "    3. Number of total year experiences\n",
    "    4. Present field of experience\n",
    "    5. Overall expertise area\n",
    "    6. Present organization designation\n",
    "    7. Research experience (any research positions, publications, or projects)\n",
    "    8. Achievements (awards, recognitions, significant accomplishments)\n",
    "    9. Mobile number\n",
    "    10. Email address\n",
    "    11. Present organization name\n",
    "    12. Working experience in present organization (start date in format 'Month YYYY', e.g. 'December 2022')\n",
    "\n",
    "    Today is {current_date_str}.\n",
    "\n",
    "    CV Text:\n",
    "    {cv_text}\n",
    "\n",
    "    Your response MUST be a valid JSON object with ONLY the following keys:\n",
    "    {{\n",
    "      \"name\": \"extracted name\",\n",
    "      \"last_education\": \"extracted education and university\",\n",
    "      \"total_experience\": \"total number of experiences in all organizations\",\n",
    "      \"present_field\": \"present field of experience\",\n",
    "      \"overall_expertise_area\": \"areas of expertise or specialization\",\n",
    "      \"present_organization_designation\": \"current job title or designation\",\n",
    "      \"research_experience\": \"details of research experience if any\",\n",
    "      \"achievements\": \"major achievements and awards if any\",\n",
    "      \"mobile\": \"extracted mobile number\",\n",
    "      \"email\": \"extracted email address\",\n",
    "      \"present_organization_name\": \"name of current organization\",\n",
    "      \"working_experience_in_present_organization\": \"start date in format 'Month YYYY'\"\n",
    "    }}\n",
    "\n",
    "    Do not include any explanation, just return the JSON object.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Use Gemini model for information extraction\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.3,  # Lower temperature for more consistent results\n",
    "                max_output_tokens=2048,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Extract the response text\n",
    "        result = response.text\n",
    "        print(result)\n",
    "\n",
    "        # Try to find JSON in the response\n",
    "        json_match = re.search(r'(\\{[\\s\\S]*\\})', result, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_str = json_match.group(1)\n",
    "            try:\n",
    "                parsed_result = json.loads(json_str)\n",
    "\n",
    "                # Calculate work experience duration\n",
    "                start_date = parsed_result.get(\"working_experience_in_present_organization\", \"Not found\")\n",
    "                parsed_result[\"working_experience_in_year_in_present_organization\"] = calculate_experience_duration(start_date)\n",
    "\n",
    "                return parsed_result\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Failed to parse JSON from response. Attempting alternate extraction.\")\n",
    "\n",
    "        # If extraction failed, try to create a structured response manually\n",
    "        try:\n",
    "            # Create a standard response manually\n",
    "            start_date = extract_field(result, \"working_experience_in_present_organization\")\n",
    "            experience_duration = calculate_experience_duration(start_date)\n",
    "\n",
    "            # Return in the specified order\n",
    "            return {\n",
    "                \"name\": extract_field(result, \"name\"),\n",
    "                \"last_education\": extract_field(result, \"last_education\"),\n",
    "                \"overall_expertise_area\": extract_field(result, \"overall_expertise_area\"),\n",
    "                \"present_organization_name\": extract_field(result, \"present_organization_name\"),\n",
    "                \"present_organization_designation\": extract_field(result, \"present_organization_designation\"),\n",
    "                \"working_experience_in_present_organization\": start_date,\n",
    "                \"working_experience_in_year_in_present_organization\": experience_duration,\n",
    "                \"total_experience\": extract_field(result, \"total_experience\"),\n",
    "                \"present_field\": extract_field(result, \"present_field\"),\n",
    "                \"research_experience\": extract_field(result, \"research_experience\"),\n",
    "                \"achievements\": extract_field(result, \"achievements\"),\n",
    "                \"mobile\": extract_field(result, \"mobile\"),\n",
    "                \"email\": extract_field(result, \"email\")\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating structured response: {str(e)}\")\n",
    "            # Last resort, try direct JSON parsing\n",
    "            parsed_result = json.loads(result)\n",
    "\n",
    "            # Calculate work experience duration\n",
    "            start_date = parsed_result.get(\"working_experience_in_present_organization\", \"Not found\")\n",
    "            parsed_result[\"working_experience_in_year_in_present_organization\"] = calculate_experience_duration(start_date)\n",
    "\n",
    "            return parsed_result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting information: {str(e)}\")\n",
    "        return {\n",
    "            \"name\": \"Error\",\n",
    "            \"last_education\": \"Error\",\n",
    "            \"total_experience\": \"Error\",\n",
    "            \"present_field\": \"Error\",\n",
    "            \"overall_expertise_area\": \"Error\",\n",
    "            \"present_organization_designation\": \"Error\",\n",
    "            \"research_experience\": \"Error\",\n",
    "            \"achievements\": \"Error\",\n",
    "            \"present_organization_name\": \"Error\",\n",
    "            \"working_experience_in_present_organization\": \"Error\",\n",
    "            \"working_experience_in_year_in_present_organization\": \"Error\",\n",
    "            \"mobile\": \"Error\",\n",
    "            \"email\": \"Error\"\n",
    "        }\n",
    "\n",
    "# Function to find all PDF files in the directory structure\n",
    "def find_pdf_files(root_path):\n",
    "    pdf_files = []\n",
    "    for root, _, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                # Get relative path components\n",
    "                rel_path = os.path.relpath(root, root_path)\n",
    "                if rel_path == '.':\n",
    "                    subfolder = \"\"\n",
    "                else:\n",
    "                    subfolder = rel_path\n",
    "\n",
    "                full_path = os.path.join(root, file)\n",
    "\n",
    "                # Get file ID and create shareable link\n",
    "                file_id = get_file_id(full_path)\n",
    "                cv_link = create_shareable_link(file_id)\n",
    "\n",
    "                pdf_files.append({\n",
    "                    'full_path': full_path,\n",
    "                    'filename': file,\n",
    "                    'root_folder': root_path,\n",
    "                    'subfolder': subfolder,\n",
    "                    'cv_link': cv_link\n",
    "                })\n",
    "    return pdf_files\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    print(f\"Starting CV analysis from root path: {root_path}\")\n",
    "\n",
    "    # Find all PDF files\n",
    "    pdf_files = find_pdf_files(root_path)\n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "\n",
    "    # Create a list to store results\n",
    "    all_results = []\n",
    "\n",
    "    # Process each PDF file\n",
    "    for pdf_info in tqdm(pdf_files, desc=\"Processing CVs\"):\n",
    "        try:\n",
    "            # Open the PDF file and extract text\n",
    "            with open(pdf_info['full_path'], 'rb') as file:\n",
    "                # Extract text from PDF\n",
    "                cv_text = extract_text_from_pdf(file)\n",
    "\n",
    "                # Extract structured information\n",
    "                cv_info = extract_cv_info(cv_text)\n",
    "\n",
    "                # Add file information\n",
    "                cv_info[\"cv_links\"] = pdf_info['filename']  # Will be displayed and renamed later\n",
    "                cv_info[\"root_folder\"] = pdf_info['root_folder']\n",
    "                cv_info[\"subfolder\"] = pdf_info['subfolder']\n",
    "                cv_info[\"_cv_link\"] = pdf_info['cv_link']  # Temporary field for processing\n",
    "\n",
    "                # Add to results list\n",
    "                all_results.append(cv_info)\n",
    "\n",
    "                # Print progress\n",
    "                print(f\"Processed: {pdf_info['filename']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_info['filename']}: {str(e)}\")\n",
    "            # Add error entry\n",
    "            error_info = {\n",
    "                \"name\": \"Error\",\n",
    "                \"last_education\": \"Error\",\n",
    "                \"total_experience\": \"Error\",\n",
    "                \"present_field\": \"Error\",\n",
    "                \"overall_expertise_area\": \"Error\",\n",
    "                \"present_organization_designation\": \"Error\",\n",
    "                \"research_experience\": \"Error\",\n",
    "                \"achievements\": \"Error\",\n",
    "                \"present_organization_name\": \"Error\",\n",
    "                \"working_experience_in_present_organization\": \"Error\",\n",
    "                \"working_experience_in_year_in_present_organization\": \"Error\",\n",
    "                \"mobile\": \"Error\",\n",
    "                \"email\": \"Error\",\n",
    "                \"cv_links\": pdf_info['filename'],\n",
    "                \"root_folder\": pdf_info['root_folder'],\n",
    "                \"subfolder\": pdf_info['subfolder'],\n",
    "                \"_cv_link\": pdf_info['cv_link']\n",
    "            }\n",
    "            all_results.append(error_info)\n",
    "\n",
    "    # Create a DataFrame from all results\n",
    "    df = pd.DataFrame(all_results)\n",
    "\n",
    "    # Define the desired column order\n",
    "    column_order = [\n",
    "        \"name\",\n",
    "        \"last_education\",\n",
    "        \"overall_expertise_area\",\n",
    "        \"present_organization_name\",\n",
    "        \"present_organization_designation\",\n",
    "        \"working_experience_in_present_organization\",\n",
    "        \"working_experience_in_year_in_present_organization\",\n",
    "        \"total_experience\",\n",
    "        \"present_field\",\n",
    "        \"research_experience\",\n",
    "        \"achievements\",\n",
    "        \"mobile\",\n",
    "        \"email\",\n",
    "        \"cv_links\",  # Renamed from filename and will be the hyperlink\n",
    "        \"root_folder\",\n",
    "        \"subfolder\"\n",
    "    ]\n",
    "\n",
    "    # Reorder columns (only include columns that exist)\n",
    "    existing_columns = [col for col in column_order if col in df.columns]\n",
    "    extra_columns = [col for col in df.columns if col not in column_order]\n",
    "    df = df[existing_columns + extra_columns]\n",
    "\n",
    "    # Ensure all data is treated as strings to avoid conversion issues\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].astype(str)\n",
    "\n",
    "    # Save the DataFrame to Excel\n",
    "    output_path = os.path.join(root_path, \"cv_analysis_results.xlsx\")\n",
    "\n",
    "    # Remove the temporary _cv_link column before saving\n",
    "    if \"_cv_link\" in df.columns:\n",
    "        df_save = df.drop(\"_cv_link\", axis=1)\n",
    "    else:\n",
    "        df_save = df.copy()\n",
    "\n",
    "    # First save using pandas to get the basic structure\n",
    "    df_save.to_excel(output_path, index=False)\n",
    "\n",
    "    # Now, modify the Excel file to create proper hyperlinks\n",
    "    workbook = openpyxl.load_workbook(output_path)\n",
    "    worksheet = workbook.active\n",
    "\n",
    "    # Find the column indices for cv_links\n",
    "    header_row = worksheet[1]\n",
    "    cv_links_col_idx = None\n",
    "\n",
    "    for idx, cell in enumerate(header_row, 1):\n",
    "        if cell.value == \"cv_links\":\n",
    "            cv_links_col_idx = idx\n",
    "\n",
    "    # If the column exists, create hyperlinks\n",
    "    if cv_links_col_idx:\n",
    "        for row_idx in range(2, worksheet.max_row + 1):\n",
    "            # Find the corresponding link from the original DataFrame\n",
    "            if row_idx - 2 < len(df) and \"_cv_link\" in df.columns:\n",
    "                file_link = df.iloc[row_idx-2][\"_cv_link\"]\n",
    "\n",
    "                # Get the filename\n",
    "                cv_links_cell = worksheet.cell(row=row_idx, column=cv_links_col_idx)\n",
    "                filename = cv_links_cell.value\n",
    "\n",
    "                if filename and file_link and file_link != \"Link not available\":\n",
    "                    # Set the hyperlink\n",
    "                    cv_links_cell.hyperlink = file_link\n",
    "                    cv_links_cell.value = filename\n",
    "\n",
    "                    # Format the cell as a hyperlink (blue and underlined)\n",
    "                    cv_links_cell.font = Font(color=\"0000FF\", underline=\"single\")\n",
    "\n",
    "    # Save the modified workbook to a final path for download\n",
    "    final_output_path = os.path.join(root_path, \"cv_analysis_results_with_links.xlsx\")\n",
    "    workbook.save(final_output_path)\n",
    "\n",
    "    print(f\"Analysis complete! Results saved to: {final_output_path} with clickable hyperlinks\")\n",
    "\n",
    "    # Automatically download the file\n",
    "    files.download(final_output_path)\n",
    "\n",
    "    return df_save\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = main()\n",
    "    display(result_df)  # Display the results in the notebook"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
