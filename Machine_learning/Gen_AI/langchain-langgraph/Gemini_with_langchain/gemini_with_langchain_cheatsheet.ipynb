{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Gemini LangChain Cheatsheet\n",
    "The langchain-google-genai provides access to Google's powerful Gemini models directly via the Gemini API & Google AI Studio. Google AI Studio enables rapid prototyping and experimentation, making it an ideal starting point for individual developers. LangChain is a framework for developing AI applications. The langchain-google-genai package connects LangChain with Google's Gemini models. LangGraph is a library for building stateful, multi-actor applications with LLMs.\n",
    "\n",
    "All examples use the gemini-2.0-flash model. Gemini 2.5 Pro and 2.5 Flash can be used via gemini-2.5-pro-preview-03-25 and gemini-2.5-flash-preview-04-17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain-google-genai\n",
      "  Downloading langchain_google_genai-2.1.4-py3-none-any.whl (44 kB)\n",
      "\u001b[K     |████████████████████████████████| 44 kB 755 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: langchain-core<0.4.0,>=0.3.52 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from langchain-google-genai) (0.3.58)\n",
      "Requirement already satisfied: pydantic<3,>=2 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from langchain-google-genai) (2.11.4)\n",
      "Collecting filetype<2.0.0,>=1.2.0\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18\n",
      "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1\n",
      "  Using cached google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (6.30.2)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1\n",
      "  Downloading google_auth-2.40.1-py2.py3-none-any.whl (216 kB)\n",
      "\u001b[K     |████████████████████████████████| 216 kB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting proto-plus<2.0.0,>=1.22.3\n",
      "  Using cached proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.32.3)\n",
      "Collecting grpcio<2.0dev,>=1.33.2\n",
      "  Using cached grpcio-1.71.0-cp39-cp39-macosx_10_14_universal2.whl (11.3 MB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2\n",
      "  Using cached grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2\n",
      "  Using cached protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (4.13.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (0.3.42)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (24.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (9.1.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (6.0.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (3.10.18)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (0.23.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (0.28.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (1.0.0)\n",
      "Requirement already satisfied: idna in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (3.10)\n",
      "Requirement already satisfied: anyio in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (1.0.9)\n",
      "Requirement already satisfied: certifi in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (2025.4.26)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (0.16.0)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (3.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/mdalamintokder/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (1.2.2)\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, protobuf, cachetools, proto-plus, grpcio, googleapis-common-protos, google-auth, grpcio-status, google-api-core, google-ai-generativelanguage, filetype, langchain-google-genai\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 6.30.2\n",
      "    Uninstalling protobuf-6.30.2:\n",
      "      Successfully uninstalled protobuf-6.30.2\n",
      "Successfully installed cachetools-5.5.2 filetype-1.2.0 google-ai-generativelanguage-0.6.18 google-api-core-2.24.2 google-auth-2.40.1 googleapis-common-protos-1.70.0 grpcio-1.71.0 grpcio-status-1.71.0 langchain-google-genai-2.1.4 proto-plus-1.26.1 protobuf-5.29.4 pyasn1-0.6.1 pyasn1-modules-0.4.2 rsa-4.9.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    " \n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Gemini with LangChain Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "আমি প্রোগ্রামিং ভালোবাসি।\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    " \n",
    "# Initialize model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    " \n",
    "# Simple invocation\n",
    "messages = [\n",
    "    (\"system\", \"You are a helpful assistant that translates English to Bangla.\"),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)  # Output: J'adore la programmation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain calls with Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "আমি প্রোগ্রামিং ভালোবাসি।\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    " \n",
    "# Initialize model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    ")\n",
    " \n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    " \n",
    "chain = prompt | llm\n",
    "result = chain.invoke({\n",
    "    \"input_language\": \"English\",\n",
    "    \"output_language\": \"Bangla\",\n",
    "    \"input\": \"I love programming.\",\n",
    "})\n",
    "print(result.content)  # Output: Ich liebe Programmieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a description of the image:\n",
      "\n",
      "**Overall Impression:** The image captures a serene and picturesque mountain landscape, likely at either sunrise or sunset, given the soft, warm light.\n",
      "\n",
      "**Foreground:** A snow-covered foreground with gentle, rolling hills. The snow appears smooth and undisturbed.\n",
      "\n",
      "**Midground:** A rocky mountain peak rises prominently. It's covered in snow and ice. The light catches the peak, highlighting its texture and form.\n",
      "\n",
      "**Background:** The sky is filled with soft, pastel-colored clouds. The colors range from light pinks and oranges to pale blues and purples, suggesting the warm light of either sunrise or sunset.\n",
      "\n",
      "**Atmosphere:** The image evokes a sense of tranquility, peace, and the grandeur of nature. The soft light and colors contribute to a calming and beautiful atmosphere.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../assets/react.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Using a local image\u001b[39;00m\n\u001b[1;32m     19\u001b[0m local_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../assets/react.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlocal_image_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m image_file:\n\u001b[1;32m     21\u001b[0m     encoded_image \u001b[38;5;241m=\u001b[39m base64\u001b[38;5;241m.\u001b[39mb64encode(image_file\u001b[38;5;241m.\u001b[39mread())\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m message_local \u001b[38;5;241m=\u001b[39m HumanMessage(\n\u001b[1;32m     24\u001b[0m     content\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     25\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescribe this image.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     26\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_url\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_url\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata:image/png;base64,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoded_image\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     27\u001b[0m     ]\n\u001b[1;32m     28\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../assets/react.png'"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "import base64\n",
    " \n",
    "# Initialize model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    " \n",
    "# Using an image URL\n",
    "message_url = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        {\"type\": \"image_url\", \"image_url\": \"https://picsum.photos/seed/picsum/200/300\"},\n",
    "    ]\n",
    ")\n",
    "result_url = llm.invoke([message_url])\n",
    "print(result_url.content)\n",
    " \n",
    "# Using a local image\n",
    "local_image_path = \"../assets/react.png\"\n",
    "with open(local_image_path, \"rb\") as image_file:\n",
    "    encoded_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    " \n",
    "message_local = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        {\"type\": \"image_url\", \"image_url\": f\"data:image/png;base64,{encoded_image}\"}\n",
    "    ]\n",
    ")\n",
    "result_local = llm.invoke([message_local])\n",
    "print(result_local.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "import base64\n",
    " \n",
    "# Initialize model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    " \n",
    "audio_file_path = \"../path/to/your/audio/file.mp3\"\n",
    "audio_mime_type = \"audio/mpeg\"\n",
    " \n",
    "with open(audio_file_path, \"rb\") as audio_file:\n",
    "    encoded_audio = base64.b64encode(audio_file.read()).decode('utf-8')\n",
    " \n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"Transcribe this audio.\"},\n",
    "        {\"type\": \"media\", \"data\": encoded_audio, \"mime_type\": audio_mime_type}\n",
    "    ]\n",
    ")\n",
    "response = llm.invoke([message])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "import base64\n",
    " \n",
    "# Initialize model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    " \n",
    "video_file_path = \"../path/to/your/video/file.mp4\"\n",
    "video_mime_type = \"video/mp4\"\n",
    " \n",
    "with open(video_file_path, \"rb\") as video_file:\n",
    "    encoded_video = base64.b64encode(video_file.read()).decode('utf-8')\n",
    " \n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"Describe what's happening in this video.\"},\n",
    "        {\"type\": \"media\", \"data\": encoded_video, \"mime_type\": video_mime_type}\n",
    "    ]\n",
    ")\n",
    "response = llm.invoke([message])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    " \n",
    "# Initialize model for image generation\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-2.0-flash-exp-image-generation\")\n",
    " \n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Generate an image of a cat wearing a hat.\",\n",
    "}\n",
    " \n",
    "response = llm.invoke(\n",
    "    [message],\n",
    "    generation_config=dict(response_modalities=[\"TEXT\", \"IMAGE\"]),\n",
    ")\n",
    " \n",
    "# Display the generated image\n",
    "image_base64 = response.content[0].get(\"image_url\").get(\"url\").split(\",\")[-1]\n",
    "image_data = base64.b64decode(image_base64)\n",
    "display(Image(data=image_data, width=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool Calling/Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import ToolMessage\n",
    " \n",
    "# Define a tool\n",
    "@tool(description=\"Get the current weather in a given location\")\n",
    "def get_weather(location: str) -> str:\n",
    "    return \"It's sunny.\"\n",
    " \n",
    "# Initialize model and bind the tool\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "llm_with_tools = llm.bind_tools([get_weather])\n",
    " \n",
    "# Invoke with a query that should trigger the tool\n",
    "query = \"What's the weather in San Francisco?\"\n",
    "ai_msg = llm_with_tools.invoke(query)\n",
    " \n",
    "# Access tool calls in the response\n",
    "print(ai_msg.tool_calls)\n",
    " \n",
    "# Pass tool results back to the model\n",
    "tool_message = ToolMessage(\n",
    "    content=get_weather(*ai_msg.tool_calls[0]['args']), \n",
    "    tool_call_id=ai_msg.tool_calls[0]['id']\n",
    ")\n",
    "final_response = llm_with_tools.invoke([ai_msg, tool_message])\n",
    "print(final_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built-in Tools (Google Search, Code Execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from google.ai.generativelanguage_v1beta.types import Tool as GenAITool\n",
    " \n",
    "# Initialize model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    " \n",
    "# Google Search\n",
    "search_resp = llm.invoke(\n",
    "    \"When is the next total solar eclipse in US?\",\n",
    "    tools=[GenAITool(google_search={})],\n",
    ")\n",
    "print(search_resp.content)\n",
    " \n",
    "# Code Execution\n",
    "code_resp = llm.invoke(\n",
    "    \"What is 2*2, use python\",\n",
    "    tools=[GenAITool(code_execution={})],\n",
    ")\n",
    " \n",
    "for c in code_resp.content:\n",
    "    if isinstance(c, dict):\n",
    "        if c[\"type\"] == 'code_execution_result':\n",
    "            print(f\"Code execution result: {c['code_execution_result']}\")\n",
    "        elif c[\"type\"] == 'executable_code':\n",
    "            print(f\"Executable code: {c['executable_code']}\")\n",
    "    else:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    " \n",
    "# Define the desired structure\n",
    "class Person(BaseModel):\n",
    "    '''Information about a person.'''\n",
    "    name: str = Field(..., description=\"The person's name\")\n",
    "    height_m: float = Field(..., description=\"The person's height in meters\")\n",
    " \n",
    "# Initialize the model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(Person)\n",
    " \n",
    "# Invoke the model with a query asking for structured information\n",
    "result = structured_llm.invoke(\"Who was the 16th president of the USA, and how tall was he in meters?\")\n",
    "print(result)  # Output: name='Abraham Lincoln' height_m=1.93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Usage Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    " \n",
    "# Initialize model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    " \n",
    "result = llm.invoke(\"Explain the concept of prompt engineering in one sentence.\")\n",
    " \n",
    "print(result.content)\n",
    "print(\"\\nUsage Metadata:\")\n",
    "print(result.usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Gemini Embeddings with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    " \n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-exp-03-07\")\n",
    " \n",
    "# Embed a single query\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    " \n",
    "# Embed multiple documents\n",
    "vectors = embeddings.embed_documents([\n",
    "    \"Today is Monday\",\n",
    "    \"Today is Tuesday\",\n",
    "    \"Today is April Fools day\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using with Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    " \n",
    "# Initialize embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-exp-03-07\")\n",
    " \n",
    "text = \"LangChain is the framework for building context-aware reasoning applications\"\n",
    " \n",
    "# Create vector store and retriever\n",
    "vectorstore = InMemoryVectorStore.from_texts([text], embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    " \n",
    "# Retrieve similar documents\n",
    "retrieved_documents = retriever.invoke(\"What is LangChain?\")\n",
    "print(retrieved_documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    " \n",
    "# Different task types for different use cases\n",
    "query_embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/gemini-embedding-exp-03-07\", \n",
    "    task_type=\"RETRIEVAL_QUERY\"  # For queries\n",
    ")\n",
    "doc_embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/gemini-embedding-exp-03-07\", \n",
    "    task_type=\"RETRIEVAL_DOCUMENT\"  # For documents\n",
    ")\n",
    " \n",
    "# Compare similarity\n",
    "q_embed = query_embeddings.embed_query(\"What is the capital of France?\")\n",
    "d_embed = doc_embeddings.embed_documents([\"The capital of France is Paris.\", \"Philipp likes to eat pizza.\"])\n",
    " \n",
    "for i, d in enumerate(d_embed):\n",
    "    similarity = cosine_similarity([q_embed], [d])[0][0]\n",
    "    print(f\"Document {i+1} similarity: {similarity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
